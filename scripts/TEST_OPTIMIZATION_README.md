# Test Suite Optimization Workflow

## Overview

This directory contains tools for systematically identifying and removing redundant/duplicate tests while maintaining code coverage within acceptable limits.

**Goal**: Remove ~40% of tests while keeping coverage drop â‰¤ 2%

## Current State

- **Total Tests**: 627 tests across 84 test files
- **Problem**: Test suite has become very slow and time-consuming
- **Hypothesis**: Significant test duplication exists

## Tools

### 1. `analyze_test_coverage.py`

Analyzes the coverage database to identify duplicate and redundant tests.

**What it does**:
- Connects to `.coverage` SQLite database generated by pytest-cov
- Extracts which lines each test covers using dynamic context tracking
- Calculates overlap percentages between all test pairs
- Identifies:
  - **Complete subsets**: Tests where one test's coverage is 100% contained in another
  - **High overlap duplicates**: Test pairs with â‰¥95% coverage overlap
  - **Removal candidates**: Prioritized list of tests that can be safely removed

**Output**:
- Detailed console report with statistics and recommendations
- `.artifacts/removal_candidates.json`: Structured data for automated removal

**Usage**:
```bash
# Must run after pytest with context tracking
poetry run python scripts/analyze_test_coverage.py
```

### 2. `remove_redundant_tests.py`

Safely removes redundant tests with validation and rollback capability.

**What it does**:
- Reads removal candidates from JSON file
- Removes tests one at a time
- After each removal, validates coverage hasn't dropped too much
- Automatically rolls back if coverage drop exceeds threshold
- Maintains backup of original test suite
- Generates detailed removal report

**Features**:
- **Iterative validation**: Tests coverage after each removal
- **Automatic rollback**: Restores tests if coverage drops too much
- **Backup preservation**: Keeps original tests in `.test_backup/`
- **Dry run mode**: Simulate removals without actually deleting
- **Detailed reporting**: Tracks what was removed and coverage impact

**Usage**:
```bash
# Dry run (recommended first)
poetry run python scripts/remove_redundant_tests.py --dry-run

# Actual removal
poetry run python scripts/remove_redundant_tests.py \
  --target-pct 40 \
  --max-drop 2.0 \
  --candidates-file removal_candidates.json
```

## Complete Workflow

### Phase 1: Enable Context Tracking & Establish Baseline

1. **Configure coverage for per-test context tracking** (already done):
   ```toml
   # In pyproject.toml
   [tool.coverage.run]
   dynamic_context = "test_function"

   [tool.coverage.html]
   show_contexts = true
   ```

2. **Run tests with context tracking**:
   ```bash
   poetry run pytest --cov=src --cov-report=html --cov-report=term
   ```

   This generates:
   - `.coverage`: SQLite database with per-test coverage data (root directory)
   - `htmlcov/`: HTML report showing which tests cover which lines (root directory)
   - `.artifacts/`: Analysis results (git-ignored)

3. **Record baseline coverage**:
   Note the coverage percentage from the terminal output.

### Phase 2: Analyze Coverage & Identify Duplicates

4. **Run coverage analysis**:
   ```bash
   poetry run python scripts/analyze_test_coverage.py
   ```

   This will:
   - Query the `.coverage` database
   - Calculate overlap between all test pairs
   - Generate `.artifacts/removal_candidates.json`
   - Print detailed report

5. **Review the analysis report**:
   - Check how many complete subsets were found
   - Review high-overlap duplicates
   - Verify removal candidates make sense

### Phase 3: Remove Redundant Tests

6. **Perform dry run** (recommended):
   ```bash
   poetry run python scripts/remove_redundant_tests.py --dry-run
   ```

   This simulates the removal without actually deleting tests.

7. **Execute actual removal**:
   ```bash
   poetry run python scripts/remove_redundant_tests.py \
     --target-pct 40 \
     --max-drop 2.0
   ```

   This will:
   - Create backup in `.test_backup/`
   - Remove tests one by one
   - Validate coverage after each removal
   - Stop if coverage drops > 2%
   - Generate `.artifacts/removed_tests.json` report

### Phase 4: Validate Results

8. **Run full test suite**:
   ```bash
   poetry run pytest --cov=src --cov-report=term
   ```

9. **Verify quality gates**:
   ```bash
   just lint-full
   just test
   ```

10. **Review results**:
    - Check final coverage drop
    - Verify all remaining tests pass
    - Review `removed_tests.json` for details

### Phase 5: Commit Changes

11. **Review changes**:
    ```bash
    git diff tests/
    git status
    ```

12. **Commit** (if satisfied):
    ```bash
    git add tests/
    git commit -m "test: Remove redundant tests to improve suite performance

    - Removed XXX redundant/duplicate tests (40% reduction)
    - Coverage drop: X.XX% (within 2% threshold)
    - Test suite run time reduced significantly
    - Used systematic coverage analysis to identify duplicates

    ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

    Co-Authored-By: Claude <noreply@anthropic.com>"
    ```

## How It Works

### Coverage Context Tracking

When you run pytest with `dynamic_context = "test_function"`, coverage.py records which test function executed each line of code. This data is stored in the `.coverage` SQLite database with this schema:

**Key Tables**:
- `file`: Maps file IDs to file paths
- `context`: Maps context IDs to test names
- `line_bits`: Links (file_id, context_id) to covered line numbers (stored as compressed "numbits")

**Example Query**:
```sql
SELECT file.path, context.context
FROM line_bits
JOIN file ON line_bits.file_id = file.id
JOIN context ON line_bits.context_id = context.id
WHERE num_in_numbits(47, numbits)  -- Find which tests cover line 47
```

### Overlap Calculation

For each test pair (test1, test2):

1. Extract covered lines: `coverage1 = {(file, line), ...}`
2. Calculate intersection: `intersection = coverage1 & coverage2`
3. Compute overlap percentage: `overlap = len(intersection) / len(coverage1) * 100`

**Duplicate Detection**:
- **100% overlap**: Test is complete subset â†’ safe to remove
- **â‰¥95% overlap**: High redundancy â†’ candidate for removal
- **<95% overlap**: Tests cover different scenarios â†’ keep both

### Safe Removal Strategy

The removal script uses an iterative approach:

```
For each candidate test:
  1. Remove test function from file
  2. Run pytest --cov to get new coverage
  3. Calculate coverage drop
  4. If drop > threshold:
       Restore test from backup
       Stop removal process
     Else:
       Keep removal
       Continue to next candidate
```

This ensures coverage never drops below the acceptable threshold.

## Configuration Options

### Analysis Script

- **`duplicate_threshold`**: Minimum overlap % to consider as duplicate (default: 95%)

### Removal Script

- **`--target-pct`**: Target % of tests to remove (default: 40%)
- **`--max-drop`**: Maximum coverage drop allowed in percentage points (default: 2.0%)
- **`--dry-run`**: Simulate removals without deleting (default: False)
- **`--candidates-file`**: JSON file with removal candidates (default: removal_candidates.json)

## Files Generated

### Root Directory
- **`.coverage`**: SQLite database with per-test coverage data
- **`htmlcov/`**: HTML report showing test-to-line mapping
- **`.test_backup/`**: Backup of original test suite

### `.artifacts/` Directory (git-ignored)
- **`removal_candidates.json`**: Prioritized list of tests to remove
- **`removed_tests.json`**: Record of what was removed and coverage impact
- **`coverage_analysis.log`**: Full analysis output
- **`DUPLICATE_TESTS_FINDINGS.md`**: Executive summary

## Troubleshooting

### "Coverage database not found"

**Problem**: `.coverage` file doesn't exist

**Solution**: Run pytest with coverage first:
```bash
poetry run pytest --cov=src --cov-report=html
```

### "Don't understand dynamic_context setting"

**Problem**: Wrong value for `dynamic_context` in `pyproject.toml`

**Solution**: Use `test_function` (underscore, not hyphen):
```toml
[tool.coverage.run]
dynamic_context = "test_function"  # Correct
# NOT: dynamic_context = "test-function"  # Wrong
```

### Tests timeout during coverage run

**Problem**: 627 tests take too long to run

**Solution**: This is exactly why we're doing this optimization! For now:
- Increase timeout in the script
- Run a subset of tests to validate the approach works
- Once redundant tests are removed, full suite will be faster

### No duplicates found

**Problem**: Analysis finds no redundant tests

**Possible reasons**:
1. Context tracking wasn't enabled during test run
2. Tests genuinely cover different code paths
3. Overlap threshold is too high (try lowering from 95% to 85%)

**Solution**: Verify context tracking is working:
```bash
# Check if contexts are in database
sqlite3 .coverage "SELECT COUNT(*) FROM context;"
# Should show > 0 contexts
```

### Coverage drops more than expected

**Problem**: Removing tests causes > 2% coverage drop

**What happens**: Script automatically rolls back and stops

**Why**: The tests weren't as redundant as overlap analysis suggested

**Options**:
1. Accept the lower target (remove fewer tests)
2. Investigate why overlap analysis was misleading
3. Increase `--max-drop` threshold (not recommended)

## Performance Metrics

**Before Optimization**:
- Tests: 627
- Run time: ~5+ minutes (times out)
- Coverage: TBD (baseline)

**After Optimization** (Target):
- Tests: ~376 (40% reduction)
- Run time: ~3 minutes (estimated)
- Coverage: Baseline - 2% (maximum)

## References

- [Coverage.py Contexts Documentation](https://coverage.readthedocs.io/en/latest/contexts.html)
- [pytest-cov Documentation](https://pytest-cov.readthedocs.io/)
- [Coverage.py Database Schema](https://coverage.readthedocs.io/en/latest/dbschema.html)

## License

Same as parent project (MIT)
